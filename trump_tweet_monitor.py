import os
import time
import logging
import random
from datetime import datetime
import json
import requests
from dotenv import load_dotenv
import feedparser
import urllib.request
import socks
import socket
from urllib.error import HTTPError

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    filename='trump_monitor.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é£ä¹¦webhook URL
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', 'https://open.feishu.cn/open-apis/bot/v2/hook/51fac4c6-a9ec-4d1e-9a8c-cad6a0be2b63')

# ä»£ç†è®¾ç½®
USE_PROXY = True  # æ˜¯å¦ä½¿ç”¨ä»£ç†
PROXY_TYPE = 'HTTP'  # ä»£ç†ç±»å‹: 'HTTP', 'SOCKS4', 'SOCKS5'
PROXY_HOST = '127.0.0.1'  # ä»£ç†ä¸»æœº
PROXY_PORT = 7890  # ä»£ç†ç«¯å£ (Clashé»˜è®¤ç«¯å£)

# è¯·æ±‚é¢‘ç‡æ§åˆ¶
MIN_REQUEST_INTERVAL = 60  # æœ€çŸ­è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
JITTER_RANGE = 30  # éšæœºæŠ–åŠ¨èŒƒå›´ï¼ˆç§’ï¼‰
last_request_time = {}  # è®°å½•æ¯ä¸ªURLçš„ä¸Šæ¬¡è¯·æ±‚æ—¶é—´

# RSS æºåœ°å€ - æ ¹æ®æµ‹è¯•è°ƒæ•´ä¼˜å…ˆçº§
TRUMP_RSS_URL = "https://nitter.privacydev.net/realDonaldTrump/rss"  # æµ‹è¯•æœ‰20æ¡å†…å®¹çš„æºä¼˜å…ˆ
# å¤‡ç”¨RSSæº
BACKUP_RSS_URLS = [
    "https://nitter.net/realDonaldTrump/rss",  # è¿æ¥æˆåŠŸä½†æ— å†…å®¹
    "https://nitter.net/JDVance/rss",  # å‰¯æ€»ç»Ÿå€™é€‰äººè´¦å·
    "https://nitter.net/realDonaldTrump45/rss",  # å¯èƒ½çš„å¤‡ç”¨è´¦å·
    "https://nitter.mehicano.me/realDonaldTrump/rss",
    "https://twitter116.com/realDonaldTrump/rss",
    "https://nitter.pussthecat.org/realDonaldTrump/rss",
    "https://nitter.1d4.us/realDonaldTrump/rss",
    "https://nitter.poast.org/realDonaldTrump/rss",  # 403
    "https://twiiit.com/realDonaldTrump/rss",  # 403
    "https://x.com/realDonaldTrump/rss"  # ç›´æ¥å°è¯•X.comï¼Œè™½ç„¶å¯èƒ½ä¸æä¾›RSS
]

# è®¾ç½®ä»£ç†
def setup_proxy():
    if USE_PROXY:
        logging.info(f"è®¾ç½®{PROXY_TYPE}ä»£ç†: {PROXY_HOST}:{PROXY_PORT}")
        
        # ä¸ºrequestsè®¾ç½®ä»£ç†
        global proxies
        if PROXY_TYPE.upper() in ['HTTP', 'HTTPS']:
            proxies = {
                'http': f'http://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'http://{PROXY_HOST}:{PROXY_PORT}'
            }
        else:  # SOCKSä»£ç†
            proxies = {
                'http': f'socks5://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'socks5://{PROXY_HOST}:{PROXY_PORT}'
            }
        
        # ä¸ºurllibå’Œfeedparserè®¾ç½®ä»£ç†
        if PROXY_TYPE.upper() == 'HTTP':
            proxy_handler = urllib.request.ProxyHandler({
                'http': f'http://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'http://{PROXY_HOST}:{PROXY_PORT}'
            })
            opener = urllib.request.build_opener(proxy_handler)
            urllib.request.install_opener(opener)
        elif PROXY_TYPE.upper() in ['SOCKS4', 'SOCKS5']:
            socks_type = socks.SOCKS5 if PROXY_TYPE.upper() == 'SOCKS5' else socks.SOCKS4
            socks.set_default_proxy(socks_type, PROXY_HOST, PROXY_PORT)
            socket.socket = socks.socksocket
        
        logging.info("ä»£ç†è®¾ç½®å®Œæˆ")

def send_to_feishu(text):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    data = {
        "msg_type": "text",
        "content": {
            "text": text
        }
    }
    try:
        # ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
        if USE_PROXY:
            response = requests.post(FEISHU_WEBHOOK_URL, json=data, proxies=proxies)
        else:
            response = requests.post(FEISHU_WEBHOOK_URL, json=data)
            
        if response.status_code != 200:
            logging.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {response.text}")
            print(f"å‘é€æ¶ˆæ¯å¤±è´¥: {response.text}")
            return False
        else:
            logging.info("æ¶ˆæ¯å‘é€æˆåŠŸ")
            print("æ¶ˆæ¯å‘é€æˆåŠŸ")
            return True
    except Exception as e:
        logging.error(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {str(e)}")
        print(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {str(e)}")
        return False

def format_tweet_message(entry):
    """æ ¼å¼åŒ–æ¨æ–‡æ¶ˆæ¯"""
    # æ·»åŠ é€‚å½“çš„å¤„ç†ä»¥æ¸…ç†é“¾æ¥æ ¼å¼
    link = entry.link
    # å¦‚æœæ˜¯Nitteré“¾æ¥ï¼Œæ·»åŠ æç¤ºï¼ŒåŒæ—¶æä¾›åŸå§‹Twitteré“¾æ¥
    if "nitter" in link:
        # ä»Nitteré“¾æ¥æå–TwitteråŸå§‹é“¾æ¥
        twitter_user = link.split("/")[-2]
        tweet_id = link.split("/")[-1]
        original_link = f"https://twitter.com/{twitter_user}/status/{tweet_id}"
        return f"""ç‰¹æœ—æ™®å‘å¸ƒæ¨æ–‡ï¼š
æ—¶é—´ï¼š{entry.published}
å†…å®¹ï¼š{entry.title}
Nitteré“¾æ¥ï¼š{link}
Twitteré“¾æ¥ï¼š{original_link}

æ³¨æ„ï¼šå¦‚æœé“¾æ¥æ— æ³•æ‰“å¼€ï¼Œå¯èƒ½æ˜¯å› ä¸ºè¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"""
    else:
        return f"""ç‰¹æœ—æ™®å‘å¸ƒæ¨æ–‡ï¼š
æ—¶é—´ï¼š{entry.published}
å†…å®¹ï¼š{entry.title}
é“¾æ¥ï¼š{link}"""

def respect_rate_limit(url):
    """å°Šé‡è¯·æ±‚é€Ÿç‡é™åˆ¶"""
    current_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…
    if url in last_request_time:
        elapsed = current_time - last_request_time[url]
        wait_time = MIN_REQUEST_INTERVAL - elapsed
        
        # æ·»åŠ éšæœºæŠ–åŠ¨ä»¥é¿å…è¯·æ±‚åŒæ­¥
        jitter = random.randint(0, JITTER_RANGE)
        wait_time += jitter
        
        if wait_time > 0:
            logging.info(f"ç­‰å¾… {wait_time:.1f} ç§’ä»¥å°Šé‡è¯·æ±‚é€Ÿç‡é™åˆ¶ ({url})")
            time.sleep(wait_time)
    
    # æ›´æ–°ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
    last_request_time[url] = time.time()

def parse_rss_with_proxy(url):
    """ä½¿ç”¨ä»£ç†è§£æRSS"""
    try:
        # å°Šé‡è¯·æ±‚é€Ÿç‡é™åˆ¶
        respect_rate_limit(url)
        
        logging.info(f"å°è¯•é€šè¿‡ä»£ç†è·å–RSS: {url}")
        if USE_PROXY:
            try:
                # ä½¿ç”¨urllibåˆ›å»ºå¸¦ä»£ç†çš„è¯·æ±‚
                req = urllib.request.Request(url)
                # æ·»åŠ User-Agentå¤´ä»¥ä¼ªè£…æˆæµè§ˆå™¨
                req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                with urllib.request.urlopen(req, timeout=15) as response:
                    rss_content = response.read()
                
                # ä½¿ç”¨å†…å®¹æ‰‹åŠ¨è§£æ
                feed = feedparser.parse(rss_content)
                # ç¡®ä¿feedæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¯¹è±¡
                if not hasattr(feed, 'entries'):
                    logging.warning(f"RSSæº {url} è¿”å›çš„å¯¹è±¡æ²¡æœ‰entrieså±æ€§")
                    # åˆ›å»ºä¸€ä¸ªç©ºçš„entriesåˆ—è¡¨
                    feed.entries = []
                return feed
            except HTTPError as e:
                if e.code == 429:
                    logging.warning(f"RSSæº {url} è¿”å›429 Too Many Requestsï¼Œå°†æ­¤æºæš‚æ—¶æ ‡è®°ä¸ºä¸å¯ç”¨")
                    # è®°å½•æ›´é•¿çš„å†·å´æ—¶é—´
                    last_request_time[url] = time.time() + 3600  # è®¾ç½®1å°æ—¶å†·å´
                    raise Exception(f"è¯·æ±‚è¿‡å¤š(429): {url}")
                else:
                    logging.error(f"HTTPé”™è¯¯: {e.code} {e.reason} - {url}")
                    raise
        else:
            # ä¸ä½¿ç”¨ä»£ç†æ—¶ç›´æ¥è°ƒç”¨ï¼Œä¹Ÿè®¾ç½®è¶…æ—¶
            feed = feedparser.parse(url, timeout=15)
            # ç¡®ä¿feedæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¯¹è±¡
            if not hasattr(feed, 'entries'):
                logging.warning(f"RSSæº {url} è¿”å›çš„å¯¹è±¡æ²¡æœ‰entrieså±æ€§")
                # åˆ›å»ºä¸€ä¸ªç©ºçš„entriesåˆ—è¡¨
                feed.entries = []
            return feed
    except Exception as e:
        logging.error(f"ä»£ç†è·å–RSSå¼‚å¸¸ ({url}): {str(e)}")
        # è¿”å›ä¸€ä¸ªæœ‰ç©ºentriesåˆ—è¡¨çš„å¯¹è±¡
        empty_feed = feedparser.FeedParserDict()
        empty_feed.entries = []
        return empty_feed

def push_recent_tweets(count=3):
    """æ¨é€æœ€è¿‘çš„æ¨æ–‡"""
    logging.info(f"å°è¯•è·å–æœ€è¿‘{count}æ¡æ¨æ–‡")
    
    # å°è¯•ä¸»RSSæº
    feed = parse_rss_with_proxy(TRUMP_RSS_URL)
    
    # æ£€æŸ¥RSSæºæ˜¯å¦æœ‰æ•ˆ
    if not feed.entries:
        logging.warning("ä¸»RSSæºæœªè¿”å›ä»»ä½•æ¡ç›®")
        
        # å°è¯•å¤‡ç”¨RSSæº
        for backup_url in BACKUP_RSS_URLS:
            logging.info(f"å°è¯•å¤‡ç”¨RSSæº: {backup_url}")
            feed = parse_rss_with_proxy(backup_url)
            if feed.entries:
                logging.info(f"å¤‡ç”¨æº {backup_url} æˆåŠŸè·å–åˆ° {len(feed.entries)} æ¡æ¨æ–‡")
                break
        
        if not feed.entries:
            logging.error("æ‰€æœ‰RSSæºå‡æœªè¿”å›æ•°æ®")
            send_to_feishu("âš ï¸ æœªèƒ½è·å–åˆ°ç‰¹æœ—æ™®çš„æœ€è¿‘æ¨æ–‡ï¼Œæ‰€æœ‰RSSæºå‡æ— è¿”å›")
            return None
    else:
        logging.info(f"æˆåŠŸè·å–åˆ° {len(feed.entries)} æ¡æ¨æ–‡")
    
    # åªå–å‰countæ¡æˆ–è€…å…¨éƒ¨ï¼ˆå¦‚æœå°‘äºcountæ¡ï¼‰
    entries = feed.entries[:min(count, len(feed.entries))]
    
    # å‘é€æ±‡æ€»æ¶ˆæ¯
    send_to_feishu(f"ğŸ“‹ ç‰¹æœ—æ™®æœ€è¿‘{len(entries)}æ¡æ¨æ–‡æ±‡æ€»ï¼š")
    
    # é€æ¡å‘é€
    for entry in entries:
        send_to_feishu(format_tweet_message(entry))
        logging.info(f"æ¨é€æ¨æ–‡: {entry.id}")
        time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿé¿å…æ¶ˆæ¯å¤ªå¿«
    
    # è¿”å›æœ€æ–°æ¨æ–‡çš„IDç”¨äºåç»­æ£€æŸ¥
    if feed.entries:
        logging.info(f"æœ€æ–°æ¨æ–‡ID: {feed.entries[0].id}")
        return feed.entries[0].id
    return None

def get_working_rss_source():
    """å°è¯•è·å–æ‰€æœ‰å¯ç”¨çš„RSSæºï¼Œè¿”å›ç¬¬ä¸€ä¸ªå·¥ä½œçš„æº"""
    # å…ˆå°è¯•ä¸»æº
    sources = [TRUMP_RSS_URL] + BACKUP_RSS_URLS
    
    for source in sources:
        try:
            feed = parse_rss_with_proxy(source)
            if feed.entries:
                return source, feed
        except Exception as e:
            logging.warning(f"æº {source} ä¸å¯ç”¨: {str(e)}")
    
    return None, None

def main():
    # è®¾ç½®ä»£ç†
    setup_proxy()
    
    # å‘é€å¯åŠ¨é€šçŸ¥
    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    proxy_info = f"ä»£ç†è®¾ç½®: {'å¯ç”¨' if USE_PROXY else 'ç¦ç”¨'}"
    startup_msg = f"ğŸ¤– ç‰¹æœ—æ™®æ¨ç‰¹ç›‘æ§æœºå™¨äººå·²å¯åŠ¨\nå¯åŠ¨æ—¶é—´ï¼š{start_time}\n{proxy_info}\nç›‘æ§é¢‘ç‡ï¼šæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"
    logging.info(startup_msg)
    send_to_feishu(startup_msg)
    print("å·²å‘é€å¯åŠ¨é€šçŸ¥")
    
    # æ¨é€æœ€è¿‘ä¸‰æ¡æ¨æ–‡å¹¶è·å–æœ€æ–°çš„æ¨æ–‡ID
    logging.info("æ­£åœ¨è·å–æœ€è¿‘ä¸‰æ¡æ¨æ–‡...")
    print("æ­£åœ¨è·å–æœ€è¿‘ä¸‰æ¡æ¨æ–‡...")
    last_entry_id = push_recent_tweets(3)
    logging.info(f"åˆå§‹æ¨æ–‡ID: {last_entry_id}")
    print(f"åˆå§‹æ¨æ–‡ID: {last_entry_id}")
    
    retry_count = 0
    last_error_time = None
    max_retry_interval = 3600  # æœ€é•¿é‡è¯•é—´éš”ä¸º1å°æ—¶
    
    # ç”¨äºè¿½è¸ªå¯ç”¨æºçš„å˜é‡
    current_source = TRUMP_RSS_URL
    
    while True:
        try:
            logging.info(f"æ£€æŸ¥æ–°æ¨æ–‡ (æº: {current_source})...")
            
            # å°è¯•å½“å‰æº
            feed = parse_rss_with_proxy(current_source)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æº
            if not feed.entries:
                logging.warning(f"å½“å‰æº {current_source} æœªè¿”å›æ•°æ®ï¼Œå°è¯•æŸ¥æ‰¾å¯ç”¨æº...")
                new_source, feed = get_working_rss_source()
                
                if new_source:
                    if new_source != current_source:
                        logging.info(f"åˆ‡æ¢åˆ°æ–°æº: {new_source}")
                        current_source = new_source
                else:
                    logging.error("æ‰€æœ‰RSSæºå‡ä¸å¯ç”¨")
                    raise Exception("æ‰€æœ‰RSSæºå‡ä¸å¯ç”¨")
            
            # é‡ç½®é‡è¯•è®¡æ•°
            if hasattr(feed, 'entries') and feed.entries:
                retry_count = 0
                
                latest_entry = feed.entries[0]
                
                # å¦‚æœæ˜¯æ–°æ¨æ–‡
                if last_entry_id != latest_entry.id:
                    new_tweet_msg = f"å‘ç°æ–°æ¨æ–‡: {latest_entry.title}"
                    logging.info(new_tweet_msg)
                    print(new_tweet_msg)
                    send_to_feishu(format_tweet_message(latest_entry))
                    last_entry_id = latest_entry.id
                    logging.info(f"æ›´æ–°æœ€æ–°æ¨æ–‡ID: {last_entry_id}")
                else:
                    logging.info("æ²¡æœ‰æ–°æ¨æ–‡")
            else:
                logging.warning("RSSæºæœªè¿”å›ä»»ä½•æ¡ç›®")
                
                # æ¯24å°æ—¶å‘é€ä¸€æ¬¡çŠ¶æ€æŠ¥å‘Š
                current_hour = datetime.now().hour
                if current_hour == 9:  # æ¯å¤©ä¸Šåˆ9ç‚¹
                    status_msg = f"ğŸ“Š ç›‘æ§çŠ¶æ€æŠ¥å‘Š: æ‰€æœ‰RSSæºç›®å‰æœªè¿”å›ä»»ä½•æ¨æ–‡ï¼Œä½†ç›‘æ§æœåŠ¡æ­£å¸¸è¿è¡Œä¸­"
                    logging.info(status_msg)
                    send_to_feishu(status_msg)
            
            # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼Œæ·»åŠ éšæœºæŠ–åŠ¨
            check_interval = 300 + random.randint(0, 60)  # 5-6åˆ†é’Ÿ
            logging.info(f"ä¼‘çœ {check_interval}ç§’...")
            time.sleep(check_interval)
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
            logging.error(error_msg)
            print(error_msg)
            
            # é™åˆ¶é”™è¯¯æ¶ˆæ¯å‘é€é¢‘ç‡
            current_time = time.time()
            if last_error_time is None or (current_time - last_error_time) > 3600:  # è‡³å°‘é—´éš”1å°æ—¶
                send_to_feishu(error_msg)
                last_error_time = current_time
            
            # ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
            retry_count += 1
            retry_delay = min(60 * (2 ** (retry_count - 1)), max_retry_interval)  # æŒ‡æ•°é€’å¢ï¼Œä½†æœ€å¤š1å°æ—¶
            
            logging.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯• (ç¬¬ {retry_count} æ¬¡é‡è¯•)")
            time.sleep(retry_delay)  # å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•

if __name__ == "__main__":
    main()