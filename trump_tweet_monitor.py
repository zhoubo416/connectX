import os
import time
import logging
from datetime import datetime
import json
import requests
from dotenv import load_dotenv
import feedparser
import urllib.request
import socks
import socket

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    filename='trump_monitor.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é£ä¹¦webhook URL
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', 'https://open.feishu.cn/open-apis/bot/v2/hook/51fac4c6-a9ec-4d1e-9a8c-cad6a0be2b63')

# ä»£ç†è®¾ç½®
USE_PROXY = True  # æ˜¯å¦ä½¿ç”¨ä»£ç†
PROXY_TYPE = 'HTTP'  # ä»£ç†ç±»å‹: 'HTTP', 'SOCKS4', 'SOCKS5'
PROXY_HOST = '127.0.0.1'  # ä»£ç†ä¸»æœº
PROXY_PORT = 7890  # ä»£ç†ç«¯å£ (Clashé»˜è®¤ç«¯å£)

# RSS æºåœ°å€ - æ ¹æ®æµ‹è¯•è°ƒæ•´ä¼˜å…ˆçº§
TRUMP_RSS_URL = "https://nitter.privacydev.net/realDonaldTrump/rss"  # æµ‹è¯•æœ‰20æ¡å†…å®¹çš„æºä¼˜å…ˆ
# å¤‡ç”¨RSSæº
BACKUP_RSS_URLS = [
    "https://nitter.net/realDonaldTrump/rss",  # è¿æ¥æˆåŠŸä½†æ— å†…å®¹
    "https://nitter.net/JDVance/rss",  # å‰¯æ€»ç»Ÿå€™é€‰äººè´¦å·
    "https://nitter.net/realDonaldTrump45/rss",  # å¯èƒ½çš„å¤‡ç”¨è´¦å·
    "https://nitter.mehicano.me/realDonaldTrump/rss",
    "https://twitter116.com/realDonaldTrump/rss",
    "https://nitter.pussthecat.org/realDonaldTrump/rss",
    "https://nitter.1d4.us/realDonaldTrump/rss",
    "https://nitter.poast.org/realDonaldTrump/rss",  # 403
    "https://twiiit.com/realDonaldTrump/rss",  # 403
    "https://x.com/realDonaldTrump/rss"  # ç›´æ¥å°è¯•X.comï¼Œè™½ç„¶å¯èƒ½ä¸æä¾›RSS
]

# è®¾ç½®ä»£ç†
def setup_proxy():
    if USE_PROXY:
        logging.info(f"è®¾ç½®{PROXY_TYPE}ä»£ç†: {PROXY_HOST}:{PROXY_PORT}")
        
        # ä¸ºrequestsè®¾ç½®ä»£ç†
        global proxies
        if PROXY_TYPE.upper() in ['HTTP', 'HTTPS']:
            proxies = {
                'http': f'http://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'http://{PROXY_HOST}:{PROXY_PORT}'
            }
        else:  # SOCKSä»£ç†
            proxies = {
                'http': f'socks5://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'socks5://{PROXY_HOST}:{PROXY_PORT}'
            }
        
        # ä¸ºurllibå’Œfeedparserè®¾ç½®ä»£ç†
        if PROXY_TYPE.upper() == 'HTTP':
            proxy_handler = urllib.request.ProxyHandler({
                'http': f'http://{PROXY_HOST}:{PROXY_PORT}',
                'https': f'http://{PROXY_HOST}:{PROXY_PORT}'
            })
            opener = urllib.request.build_opener(proxy_handler)
            urllib.request.install_opener(opener)
        elif PROXY_TYPE.upper() in ['SOCKS4', 'SOCKS5']:
            socks_type = socks.SOCKS5 if PROXY_TYPE.upper() == 'SOCKS5' else socks.SOCKS4
            socks.set_default_proxy(socks_type, PROXY_HOST, PROXY_PORT)
            socket.socket = socks.socksocket
        
        logging.info("ä»£ç†è®¾ç½®å®Œæˆ")

def send_to_feishu(text):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    data = {
        "msg_type": "text",
        "content": {
            "text": text
        }
    }
    try:
        # ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
        if USE_PROXY:
            response = requests.post(FEISHU_WEBHOOK_URL, json=data, proxies=proxies)
        else:
            response = requests.post(FEISHU_WEBHOOK_URL, json=data)
            
        if response.status_code != 200:
            logging.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {response.text}")
            print(f"å‘é€æ¶ˆæ¯å¤±è´¥: {response.text}")
            return False
        else:
            logging.info("æ¶ˆæ¯å‘é€æˆåŠŸ")
            print("æ¶ˆæ¯å‘é€æˆåŠŸ")
            return True
    except Exception as e:
        logging.error(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {str(e)}")
        print(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {str(e)}")
        return False

def format_tweet_message(entry):
    """æ ¼å¼åŒ–æ¨æ–‡æ¶ˆæ¯"""
    return f"""ç‰¹æœ—æ™®å‘å¸ƒæ¨æ–‡ï¼š
æ—¶é—´ï¼š{entry.published}
å†…å®¹ï¼š{entry.title}
é“¾æ¥ï¼š{entry.link}"""

def parse_rss_with_proxy(url):
    """ä½¿ç”¨ä»£ç†è§£æRSS"""
    try:
        logging.info(f"å°è¯•é€šè¿‡ä»£ç†è·å–RSS: {url}")
        if USE_PROXY:
            # ä½¿ç”¨urllibåˆ›å»ºå¸¦ä»£ç†çš„è¯·æ±‚
            req = urllib.request.Request(url)
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
            with urllib.request.urlopen(req, timeout=15) as response:
                rss_content = response.read()
            # ä½¿ç”¨å†…å®¹æ‰‹åŠ¨è§£æ
            return feedparser.parse(rss_content)
        else:
            # ä¸ä½¿ç”¨ä»£ç†æ—¶ç›´æ¥è°ƒç”¨ï¼Œä¹Ÿè®¾ç½®è¶…æ—¶
            return feedparser.parse(url, timeout=15)
    except Exception as e:
        logging.error(f"ä»£ç†è·å–RSSå¼‚å¸¸ ({url}): {str(e)}")
        return feedparser.FeedParserDict()

def push_recent_tweets(count=3):
    """æ¨é€æœ€è¿‘çš„æ¨æ–‡"""
    logging.info(f"å°è¯•è·å–æœ€è¿‘{count}æ¡æ¨æ–‡")
    
    # å°è¯•ä¸»RSSæº
    feed = parse_rss_with_proxy(TRUMP_RSS_URL)
    
    # æ£€æŸ¥RSSæºæ˜¯å¦æœ‰æ•ˆ
    if not feed.entries:
        logging.warning("ä¸»RSSæºæœªè¿”å›ä»»ä½•æ¡ç›®")
        
        # å°è¯•å¤‡ç”¨RSSæº
        for backup_url in BACKUP_RSS_URLS:
            logging.info(f"å°è¯•å¤‡ç”¨RSSæº: {backup_url}")
            feed = parse_rss_with_proxy(backup_url)
            if feed.entries:
                logging.info(f"å¤‡ç”¨æº {backup_url} æˆåŠŸè·å–åˆ° {len(feed.entries)} æ¡æ¨æ–‡")
                break
        
        if not feed.entries:
            logging.error("æ‰€æœ‰RSSæºå‡æœªè¿”å›æ•°æ®")
            send_to_feishu("âš ï¸ æœªèƒ½è·å–åˆ°ç‰¹æœ—æ™®çš„æœ€è¿‘æ¨æ–‡ï¼Œæ‰€æœ‰RSSæºå‡æ— è¿”å›")
            return None
    else:
        logging.info(f"æˆåŠŸè·å–åˆ° {len(feed.entries)} æ¡æ¨æ–‡")
    
    # åªå–å‰countæ¡æˆ–è€…å…¨éƒ¨ï¼ˆå¦‚æœå°‘äºcountæ¡ï¼‰
    entries = feed.entries[:min(count, len(feed.entries))]
    
    # å‘é€æ±‡æ€»æ¶ˆæ¯
    send_to_feishu(f"ğŸ“‹ ç‰¹æœ—æ™®æœ€è¿‘{len(entries)}æ¡æ¨æ–‡æ±‡æ€»ï¼š")
    
    # é€æ¡å‘é€
    for entry in entries:
        send_to_feishu(format_tweet_message(entry))
        logging.info(f"æ¨é€æ¨æ–‡: {entry.id}")
        time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿé¿å…æ¶ˆæ¯å¤ªå¿«
    
    # è¿”å›æœ€æ–°æ¨æ–‡çš„IDç”¨äºåç»­æ£€æŸ¥
    if feed.entries:
        logging.info(f"æœ€æ–°æ¨æ–‡ID: {feed.entries[0].id}")
        return feed.entries[0].id
    return None

def main():
    # è®¾ç½®ä»£ç†
    setup_proxy()
    
    # å‘é€å¯åŠ¨é€šçŸ¥
    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    proxy_info = f"ä»£ç†è®¾ç½®: {'å¯ç”¨' if USE_PROXY else 'ç¦ç”¨'}"
    startup_msg = f"ğŸ¤– ç‰¹æœ—æ™®æ¨ç‰¹ç›‘æ§æœºå™¨äººå·²å¯åŠ¨\nå¯åŠ¨æ—¶é—´ï¼š{start_time}\n{proxy_info}\nç›‘æ§é¢‘ç‡ï¼šæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"
    logging.info(startup_msg)
    send_to_feishu(startup_msg)
    print("å·²å‘é€å¯åŠ¨é€šçŸ¥")
    
    # æ¨é€æœ€è¿‘ä¸‰æ¡æ¨æ–‡å¹¶è·å–æœ€æ–°çš„æ¨æ–‡ID
    logging.info("æ­£åœ¨è·å–æœ€è¿‘ä¸‰æ¡æ¨æ–‡...")
    print("æ­£åœ¨è·å–æœ€è¿‘ä¸‰æ¡æ¨æ–‡...")
    last_entry_id = push_recent_tweets(3)
    logging.info(f"åˆå§‹æ¨æ–‡ID: {last_entry_id}")
    print(f"åˆå§‹æ¨æ–‡ID: {last_entry_id}")
    
    while True:
        try:
            logging.info("æ£€æŸ¥æ–°æ¨æ–‡...")
            # è·å– RSS æºå†…å®¹
            feed = parse_rss_with_proxy(TRUMP_RSS_URL)
            
            if feed.entries:
                latest_entry = feed.entries[0]
                
                # å¦‚æœæ˜¯æ–°æ¨æ–‡
                if last_entry_id != latest_entry.id:
                    new_tweet_msg = f"å‘ç°æ–°æ¨æ–‡: {latest_entry.title}"
                    logging.info(new_tweet_msg)
                    print(new_tweet_msg)
                    send_to_feishu(format_tweet_message(latest_entry))
                    last_entry_id = latest_entry.id
                    logging.info(f"æ›´æ–°æœ€æ–°æ¨æ–‡ID: {last_entry_id}")
                else:
                    logging.info("æ²¡æœ‰æ–°æ¨æ–‡")
            else:
                logging.warning("RSSæºæœªè¿”å›ä»»ä½•æ¡ç›®")
                
                # å°è¯•å¤‡ç”¨RSSæº
                success = False
                for backup_url in BACKUP_RSS_URLS:
                    feed = parse_rss_with_proxy(backup_url)
                    if feed.entries:
                        if last_entry_id != feed.entries[0].id:
                            new_tweet_msg = f"å‘ç°æ–°æ¨æ–‡(æ¥è‡ªå¤‡ç”¨æº): {feed.entries[0].title}"
                            logging.info(new_tweet_msg)
                            print(new_tweet_msg)
                            send_to_feishu(format_tweet_message(feed.entries[0]))
                            last_entry_id = feed.entries[0].id
                        success = True
                        break
                
                if not success:
                    # æ¯24å°æ—¶å‘é€ä¸€æ¬¡çŠ¶æ€æŠ¥å‘Š
                    current_hour = datetime.now().hour
                    if current_hour == 9:  # æ¯å¤©ä¸Šåˆ9ç‚¹
                        status_msg = f"ğŸ“Š ç›‘æ§çŠ¶æ€æŠ¥å‘Š: æ‰€æœ‰RSSæºç›®å‰æœªè¿”å›ä»»ä½•æ¨æ–‡ï¼Œä½†ç›‘æ§æœåŠ¡æ­£å¸¸è¿è¡Œä¸­"
                        logging.info(status_msg)
                        send_to_feishu(status_msg)
            
            # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            logging.info("ä¼‘çœ 5åˆ†é’Ÿ...")
            time.sleep(300)
            
        except Exception as e:
            error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
            logging.error(error_msg)
            print(error_msg)
            send_to_feishu(error_msg)
            time.sleep(60)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…1åˆ†é’Ÿåé‡è¯•

if __name__ == "__main__":
    main()